{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai, os\n",
    "import gradio as gr\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(max_tokens=2048, temperature=0.5), \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "def predict(input, history=[]):\n",
    "    history.append(input)\n",
    "    response = conversation.predict(input=input)\n",
    "    history.append(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上语音输入功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai, os\n",
    "import gradio as gr\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(max_tokens=2048, temperature=0.5), \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "def predict(input, history=[]):\n",
    "    history.append(input)\n",
    "    response = conversation.predict(input=input)\n",
    "    history.append(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "\n",
    "def transcribe(audio):\n",
    "    os.rename(audio, audio + '.wav')\n",
    "    audio_file = open(audio + '.wav', \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "    return transcript['text']    \n",
    "\n",
    "def process_audio(audio, history=[]):\n",
    "    text = transcribe(audio)\n",
    "    return predict(text, history)\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "        \n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "    audio.change(process_audio, [audio, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai, os\n",
    "import gradio as gr\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(max_tokens=2048, temperature=0.5), \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('AZURE_SPEECH_KEY'), region=os.environ.get('AZURE_SPEECH_REGION'))\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The language of the voice that speaks.\n",
    "speech_config.speech_synthesis_language='zh-CN'\n",
    "speech_config.speech_synthesis_voice_name='zh-CN-XiaohanNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "def play_voice(text):\n",
    "    speech_synthesizer.speak_text_async(text)\n",
    "\n",
    "def predict(input, history=[]):\n",
    "    history.append(input)\n",
    "    response = conversation.predict(input=input)\n",
    "    history.append(response)\n",
    "    play_voice(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "\n",
    "def transcribe(audio):\n",
    "    os.rename(audio, audio + '.wav')\n",
    "    audio_file = open(audio + '.wav', \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "    return transcript['text']    \n",
    "\n",
    "def process_audio(audio, history=[]):\n",
    "    text = transcribe(audio)\n",
    "    return predict(text, history)\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "        \n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "    audio.change(process_audio, [audio, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'tlk_VpRbkNLayqT6CQbNseW0D', 'created_at': '2023-04-12T14:42:03.971Z', 'created_by': 'google-oauth2|103752135956955592319', 'status': 'created', 'object': 'talk'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def generate_talk(input, avatar_url, \n",
    "                  voice_type = \"microsoft\", \n",
    "                  voice_id = \"zh-CN-YunyeNeural\", \n",
    "                  api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks\"\n",
    "    payload = {\n",
    "        \"script\": {\n",
    "            \"type\": \"text\",\n",
    "            \"provider\": {\n",
    "                \"type\": voice_type,\n",
    "                \"voice_id\": voice_id\n",
    "            },\n",
    "            \"ssml\": \"false\",\n",
    "            \"input\": input\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"fluent\": \"false\",\n",
    "            \"pad_audio\": \"0.0\"\n",
    "        },\n",
    "        \"source_url\": avatar_url\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": \"Basic \" + api_key\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "avatar_url = \"https://cdn.discordapp.com/attachments/1065596492796153856/1095617463112187984/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.png\"\n",
    "text = \"今天天气真不错呀。\"\n",
    "\n",
    "response = generate_talk(input=text, avatar_url=avatar_url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'driver_url': 'bank://lively/driver-01/original', 'mouth_open': False, 'num_faces': 1, 'num_frames': 48, 'processing_fps': 22.723399420553317, 'resolution': [512, 512], 'size_kib': 463.5166015625}, 'audio_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103752135956955592319/tlk_IoaVpU_FjWDh0Q5suHm79/microsoft.wav?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1681372446&Signature=9rHsfe4DGFl7lAukrxVJLILNCb0%3D', 'created_at': '2023-04-12T07:54:05.252Z', 'face': {'mask_confidence': -1, 'detection': [302, 224, 754, 795], 'overlap': 'no', 'size': 845, 'top_left': [106, 87], 'face_id': 0, 'detect_confidence': 0.9999815225601196}, 'config': {'stitch': False, 'pad_audio': 0, 'align_driver': True, 'sharpen': True, 'auto_match': True, 'normalization_factor': 1, 'logo': {'url': 'd-id-logo', 'position': [0, 0]}, 'motion_factor': 1, 'result_format': '.mp4', 'fluent': False, 'align_expand_factor': 0.3}, 'source_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103752135956955592319/tlk_IoaVpU_FjWDh0Q5suHm79/source/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.png?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1681372446&Signature=e4rHZV8LUej4BnFfjhBKkoNYFyo%3D', 'created_by': 'google-oauth2|103752135956955592319', 'status': 'done', 'driver_url': 'bank://lively/', 'modified_at': '2023-04-12T07:54:08.989Z', 'user_id': 'google-oauth2|103752135956955592319', 'result_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103752135956955592319/tlk_IoaVpU_FjWDh0Q5suHm79/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1681372448&Signature=UZ7h64t05MfE%2FvZkvXpwopgD74U%3D', 'id': 'tlk_IoaVpU_FjWDh0Q5suHm79', 'duration': 2, 'started_at': '2023-04-12T07:54:06.798'}\n"
     ]
    }
   ],
   "source": [
    "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks/\" + id\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": \"Basic \"+api_key\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "talk = get_a_talk(response['id'])\n",
    "print(talk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103752135956955592319/tlk_IoaVpU_FjWDh0Q5suHm79/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1681372448&Signature=UZ7h64t05MfE%2FvZkvXpwopgD74U%3D\n"
     ]
    }
   ],
   "source": [
    "print(talk['result_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"320\" height=\"240\" controls>\n",
       "        <source src=\"https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103752135956955592319/tlk_IoaVpU_FjWDh0Q5suHm79/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1681372448&Signature=UZ7h64t05MfE%2FvZkvXpwopgD74U%3D\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "def play_mp4_video(url):\n",
    "    video_tag = f\"\"\"\n",
    "    <video width=\"320\" height=\"240\" controls>\n",
    "        <source src=\"{url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"\n",
    "    return HTML(video_tag)\n",
    "result_url = talk['result_url']\n",
    "play_mp4_video(result_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuwenhao/miniconda3/envs/geektime/lib/python3.10/site-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in HTML, please remove them: {'live': False}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuwenhao/miniconda3/envs/geektime/lib/python3.10/site-packages/gradio/processing_utils.py:239: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "/Users/xuwenhao/miniconda3/envs/geektime/lib/python3.10/site-packages/gradio/processing_utils.py:239: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "import openai, os, time, requests\n",
    "import gradio as gr\n",
    "from gradio import HTML\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(max_tokens=2048, temperature=0.5), \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "avatar_url = \"https://cdn.discordapp.com/attachments/1065596492796153856/1095617463112187984/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.png\"\n",
    "\n",
    "def generate_talk(input, avatar_url, \n",
    "                  voice_type = \"microsoft\", \n",
    "                  voice_id = \"zh-CN-YunyeNeural\", \n",
    "                  api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks\"\n",
    "    payload = {\n",
    "        \"script\": {\n",
    "            \"type\": \"text\",\n",
    "            \"provider\": {\n",
    "                \"type\": voice_type,\n",
    "                \"voice_id\": voice_id\n",
    "            },\n",
    "            \"ssml\": \"false\",\n",
    "            \"input\": input\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"fluent\": \"false\",\n",
    "            \"pad_audio\": \"0.0\"\n",
    "        },\n",
    "        \"source_url\": avatar_url\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": \"Basic \" + api_key\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks/\" + id\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": \"Basic \"+api_key\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_mp4_video(input, avatar_url=avatar_url):\n",
    "    response = generate_talk(input=input, avatar_url=avatar_url)\n",
    "    talk = get_a_talk(response['id'])\n",
    "    video_url = \"\"\n",
    "    index = 0\n",
    "    while index < 30:\n",
    "        index += 1\n",
    "        if 'result_url' in talk:    \n",
    "            video_url = talk['result_url']\n",
    "            return video_url\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "            talk = get_a_talk(response['id'])\n",
    "    return video_url\n",
    "\n",
    "def predict(input, history=[]):\n",
    "    if input is not None:\n",
    "        history.append(input)\n",
    "        response = conversation.predict(input=input)    \n",
    "        video_url = get_mp4_video(input=response, avatar_url=avatar_url)\n",
    "        video_html = f\"\"\"<video width=\"320\" height=\"240\" controls autoplay><source src=\"{video_url}\" type=\"video/mp4\"></video>\"\"\"\n",
    "        history.append(response)\n",
    "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "        return responses, video_html, history\n",
    "    else:\n",
    "        video_html = f'<img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"John Carmack\">'\n",
    "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "        return responses, video_html, history\n",
    "        \n",
    "        \n",
    "\n",
    "def transcribe(audio):\n",
    "    os.rename(audio, audio + '.wav')\n",
    "    audio_file = open(audio + '.wav', \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, prompt=\"这是一段简体中文的问题。\")\n",
    "    return transcript['text']    \n",
    "\n",
    "def process_audio(audio, history=[]):\n",
    "    if audio is not None:\n",
    "        text = transcribe(audio)\n",
    "        return predict(text, history)\n",
    "    else:\n",
    "        text = None\n",
    "        return predict(text, history)\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:500px} .overflow-y-auto{height:500px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "\n",
    "    with gr.Row():\n",
    "        video = gr.HTML(f'<img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"John Carmack\">', live=False)\n",
    "\n",
    "    txt.submit(predict, [txt, state], [chatbot, video, state])\n",
    "    audio.change(process_audio, [audio, state], [chatbot, video, state])\n",
    "    \n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
